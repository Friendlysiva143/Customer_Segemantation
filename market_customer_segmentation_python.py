# -*- coding: utf-8 -*-
"""Market_Customer_segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ide1i954p6uSQKhmUPKZ7iKLU8Uyfx-d
"""

from google.colab import drive
drive.mount('/content/drive')

from datetime import datetime
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from scipy import stats
from scipy.stats import ttest_ind
import warnings
warnings.filterwarnings('ignore')

df=pd.read_excel("/content/drive/MyDrive/marketing_campaign.xlsx",sheet_name="marketing_campaign",parse_dates=['Dt_Customer'])

data_Original=df.copy()

df.head()

df.tail()

df.shape

df.info()

df.describe()

df.isnull().sum()

df.dtypes

df.duplicated().sum()

df.columns.duplicated().sum()

df.corr(numeric_only=True)

plt.figure(figsize=(17,17))
sns.heatmap(df.corr(numeric_only=True),annot=True,cmap='coolwarm',linewidths=0.5)
plt.show()

"""# UniVariate Analysis"""

df.columns

df['ID'].nunique()

df['Year_Birth']

df['Year_Birth'].dtype

df['Year_Birth'].describe()

df['Age']=datetime.now().year-df['Year_Birth']

df['Age'].head()



df['Age'].describe()

df['Age'].dtype

sns.histplot(df['Age'],kde=True)
plt.show()

age_box=plt.boxplot(df['Age'])
plt.show()

age_out=[]
for i in age_box['fliers']:
    age_out.append(i.get_ydata())
age_out

df[df['Age']>100]

Q1 = df['Age'].quantile(0.25)
Q3 = df['Age'].quantile(0.75)
IQR = Q3 - Q1
# Define lower and upper limits
lower_limit = Q1 - 1.5 * IQR
upper_limit = Q3 + 1.5 * IQR
# Cap values
df['Age'] = np.where(df['Age'] > upper_limit, upper_limit,
           np.where(df['Age'] < lower_limit, lower_limit, df['Age']))

age_box=plt.boxplot(df['Age'])
plt.show()

df.columns

df['Education']

df['Education'].unique()

plt.figure(figsize=(10,7))
sns.countplot(x='Education',data=df,palette='Set2')
plt.title('Education vs Count')
plt.show()





df['Marital_Status'].unique()

df['Marital_Status'].value_counts()

counts = df['Marital_Status'].value_counts()
plt.bar(counts.index, counts.values, color='skyblue')
plt.title('Count of Marital Status')
plt.xlabel('Marital Status')
plt.ylabel('Count')
plt.show()

df['Marital_Status'] = df['Marital_Status'].replace({
    'Alone': 'Single',       # treat alone as single
    'Absurd': np.nan,        # invalid
    'YOLO': np.nan           # invalid
})

df['Marital_Status'].unique()

df['Marital_Status'].isnull().sum()

df['Marital_Status'].value_counts()

mar_mode=df['Marital_Status'].mode()[0]
mar_mode

df['Marital_Status']=df['Marital_Status'].fillna(mar_mode)

df['Marital_Status'].isnull().sum()

counts = df['Marital_Status'].value_counts()
plt.bar(counts.index, counts.values, color='skyblue')
plt.title('Count of Marital Status')
plt.xlabel('Marital Status')
plt.ylabel('Count')
plt.show()

df['Income'].head()

df['Income'].dtype

df['Income'].describe()

df['Income'].isnull().sum()

edu=df.groupby('Education')['Income'].agg(['min','max','mean']).reset_index()
print(edu)

# Group by Martial Status and find Min & Max Income
result = df.groupby('Marital_Status').agg(
    Min_Salary=('Income', 'min'),
    Max_Salary=('Income', 'max'),
    mean_salary=('Income','mean')
).reset_index()

print(result)



result = df.groupby(['Marital_Status', 'Education']).agg(
    Min_Income=('Income', 'min'),
    Max_Income=('Income', 'max'),
    Mean_Income=('Income', 'mean'),
    Median_Income=('Income', 'median')
).reset_index()
print(result)

cross_tab = pd.crosstab(df['Marital_Status'], df['Education'])
cross_tab.plot(kind='bar', stacked=True, figsize=(8,5), colormap='Set2')
plt.title('Marital Status vs Education')
plt.xlabel('Marital Status')
plt.ylabel('Count')
plt.show()

sns.countplot(x='Marital_Status', hue='Education', data=df, palette='Set2')
plt.title('Marital Status vs Education')
plt.show()



df['Income'].isnull().sum()

# Fill NaN in Income with the mean income for each Marital_Status + Education group
df['Income'] = df['Income'].fillna(
    df.groupby(['Marital_Status', 'Education'])['Income'].transform('mean')
)

df['Income'].isnull().sum()

plt.figure(figsize=(8,5))
sns.histplot(df['Income'],bins=10, kde=True, color='skyblue')
plt.title('Income Distribution')
plt.xlabel('Income')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(6,5))
sns.boxplot(y=df['Income'], color='lightgreen')
plt.title('Income Boxplot')
plt.ylabel('Income')
plt.show()

Q1=df['Income'].quantile(0.25)
Q3=df['Income'].quantile(0.75)
IQR=Q3-Q1
lower_bound=Q1-1.5*IQR
upper_bound=Q3+1.5*IQR

print(lower_bound)
print(upper_bound)

# Income can't be negative So
lower_bound=0

income_box=df[(df['Income']<lower_bound) | (df['Income']>upper_bound)]
income_box['Income']

len(income_box)



# As per the dataset is about the customers can have income ~ 150000-200000
# So we are going to cap the values that are greater than 200000

df.loc[df['Income']>=upper_bound,'Income'] = upper_bound

df['Income'].describe()

df['Income'].skew()

plt.figure(figsize=(8,5))
sns.histplot(df['Income'], bins=10, kde=True, color='skyblue')
plt.title('Income Distribution')
plt.xlabel('Income')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(6,5))
sns.boxplot(y=df['Income'], color='lightgreen')
plt.title('Income Boxplot')
plt.ylabel('Income')
plt.show()

plt.figure(figsize=(6,5))
sns.violinplot(y=df['Income'], color='lightcoral')
plt.title('Income Violin Plot')
plt.show()

df.columns

df['Kidhome'].head()

df['Kidhome'].dtype

df['Kidhome']=df['Kidhome'].astype('int64')

df['Kidhome'].dtype

df['Kidhome'].value_counts()

# Vertical countplot
plt.figure(figsize=(8,5))
sns.countplot(x='Kidhome', data=df, palette='Set2')
plt.title('Count of Number of Kids')
plt.xlabel('No.Of Kids')
plt.ylabel('Count')
plt.show()

df['Teenhome'].head()

df['Teenhome'].dtype

df['Teenhome']=df['Teenhome'].astype('int64')

df['Teenhome'].dtype

df['Teenhome'].value_counts()

ax = sns.countplot(x='Teenhome', data=df, palette='Set1')
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width()/2., p.get_height()),
                ha='center', va='bottom')
plt.show()

# Define a function for FamilySize
def calculate_family_size(row):
    base = row['Kidhome'] + row['Teenhome']
    if row['Marital_Status'] in ['Single','Divorced','Widow']:
        return base + 1
    elif row['Marital_Status'] in ['Together', 'Married']:
        return base + 2
# Apply function row-wise
df['FamilySize'] = df.apply(calculate_family_size, axis=1)
# Check
print(df[['Kidhome','Teenhome','Marital_Status','FamilySize']].head())

df.columns

# Date the customer enrolled or became a client.
df['Dt_Customer']

df['Dt_Customer'].dtype



df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], errors='coerce')

df['Dt_Customer'].describe()

#Number of days since the customer’s last purchase
df['Recency']

df['Recency'].dtype

df['Recency'].describe()

plt.figure(figsize=(8,5))
sns.histplot(df['Recency'], bins=10, kde=True, color='skyblue')
plt.title('Recency Distribution')
plt.xlabel('Recency')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(6,5))
sns.boxplot(y=df['Recency'], color='lightgreen')
plt.title('Recency Boxplot')
plt.ylabel('Recency')
plt.show()

plt.figure(figsize=(6,5))
sns.violinplot(y=df['Recency'], color='lightcoral')
plt.title('Recency Violin Plot')
plt.show()

Q1=df['Recency'].quantile(0.25)
Q3=df['Recency'].quantile(0.75)
IQR=Q3-Q1
lower_bound=0
upper_bound=Q3+1.5*IQR

print(lower_bound,upper_bound)

outliers=df[(df['Recency']<lower_bound) | (df['Recency']>upper_bound)]

outliers # Since there are no outliers



# amount spent on the Wines
df['MntWines']

df['MntWines'].dtype

df['MntWines'].describe()

plt.figure(figsize=(8,5))
sns.histplot(df['MntWines'], bins=10, kde=True, color='skyblue')
plt.title('MntWines Distribution')
plt.xlabel('MntWines')
plt.ylabel('Count')
plt.show()
# Right skewed(+ve Skew

plt.figure(figsize=(6,5))
sns.boxplot(y=df['MntWines'], color='lightgreen')
plt.title('MntWines Boxplot')
plt.ylabel('MntWines')
plt.show()

df['MntWines'].skew()

df['MntWines_transformed']=np.sqrt(df['MntWines'])

df['MntWines_transformed'].skew()

sns.histplot(df['MntWines_transformed'], kde=True)
plt.title('Sqrt-transformed MntWines Distribution')
plt.show()

df['MntWines_transformed'].describe()

df.columns

# Amount spent on fruits.
df['MntFruits']

df['MntFruits'].info()

df['MntFruits'].describe()

df['MntFruits'].dtype

plt.figure(figsize=(10,6))
sns.histplot(df['MntFruits'],bins=10,kde=True)
plt.show()

plt.figure(figsize=(10,7))
fruits_box=plt.boxplot(df['MntFruits'])
plt.show()

# saving the outliers in a list
out=[]
for i in fruits_box['fliers']:
    out.append(i.get_ydata())
out

len(out[0])

df['MntFruits'].skew()

df['MntFruits_transformed']=np.log1p(df['MntFruits'])

plt.hist(df['MntFruits_transformed'],bins=10,color='skyblue')
plt.show()

df['MntFruits_transformed'].skew()

sns.boxplot(df['MntFruits_transformed'])
plt.show()

df['MntFruits_transformed'].describe()

# Amount spent on meat products.
df['MntMeatProducts']

df['MntMeatProducts'].describe()

df['MntMeatProducts'].dtype

df['MntMeatProducts'].info()

plt.figure(figsize=(10,6))
sns.histplot(x=df['MntMeatProducts'],bins=10,kde=True,color='skyblue')
plt.show()

plt.figure(figsize=(10,7))
meat_box=plt.boxplot(df['MntMeatProducts'])
plt.show()

df['MntMeatProducts'].skew()

meat_out=[]
for i in meat_box['fliers']:
    meat_out.append(i.get_ydata())
print(len(meat_out[0]))
meat_out

df['MntMeatProducts_transformed']=np.log1p(df['MntMeatProducts'])

df['MntMeatProducts_transformed'].skew()

plt.figure(figsize=(10,6))
sns.histplot(x=df['MntMeatProducts_transformed'],bins=10,kde=True,color='skyblue')
plt.show()

plt.figure(figsize=(10,7))
meat_box=plt.boxplot(df['MntMeatProducts_transformed'])
plt.show()

df.columns

df['MntFishProducts']

df['MntFishProducts'].describe()

df['MntFishProducts'].dtype

df['MntFishProducts'].skew()

plt.figure(figsize=(10,6))
sns.histplot(x=df['MntFishProducts'],bins=10,kde=True,color='skyblue')
plt.show()

plt.figure(figsize=(10,7))
fish_box=plt.boxplot(df['MntFishProducts'])
plt.show()

sns.violinplot(df['MntFishProducts'])

fish_out=[]
for i in fish_box['fliers']:
    fish_out.append(i.get_ydata())
fish_out

len(fish_out[0])

df['MntFishProducts_transformed']=np.log1p(df['MntFishProducts'])

plt.figure(figsize=(10,7))
fish_box=plt.boxplot(df['MntFishProducts_transformed'])
plt.show()

plt.figure(figsize=(10,6))
sns.histplot(x=df['MntFishProducts_transformed'],bins=10,kde=True,color='skyblue')
plt.show()

df['MntFishProducts_transformed'].describe()

#Amount spent on sweets.
df['MntSweetProducts']

df['MntSweetProducts'].describe()

df['MntSweetProducts'].dtype

def visualize_column(df, column):
    #Plots Histogram, Boxplot, and Violin Plot for one numeric column
    # Create the 3 plots
    plt.figure(figsize=(15, 6))
    # Histogram
    plt.subplot(1, 3, 1)
    sns.histplot(df[column], kde=True, bins=10)
    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    # Boxplot
    plt.subplot(1, 3, 2)
    sns.boxplot(x=df[column], fliersize=3)
    plt.title(f'Boxplot of {column}')
    plt.xlabel(column)
    # Violin Plot
    plt.subplot(1, 3, 3)
    sns.violinplot(x=df[column], inner='box')
    plt.title(f'Violin Plot of {column}')
    plt.xlabel(column)
    plt.tight_layout()
    plt.show()

plt.figure(figsize=(15,10))

visualize_column(df,'MntSweetProducts')

df['MntSweetProducts'].skew()

Q1=df['MntSweetProducts'].quantile(0.25)
Q3=df['MntSweetProducts'].quantile(0.75)
IQR=Q3-Q1
lower_bound=0
upper_bound=Q3+1.5*IQR

print(upper_bound)

sweet_out=df[(df['MntSweetProducts']<lower_bound) | (df['MntSweetProducts']>upper_bound)]

sweet_out['MntSweetProducts']

df['MntSweetProducts_transformed']=np.log1p(df['MntSweetProducts'])
df['MntSweetProducts_transformed'].skew()

visualize_column(df,'MntSweetProducts_transformed')

df.columns

# Amount spent on gold or luxury products.
df['MntGoldProds']

df['MntGoldProds'].dtype

df['MntGoldProds'].describe()

df['MntGoldProds'].skew()

visualize_column(df,'MntGoldProds')

Q1=df['MntGoldProds'].quantile(0.25)
Q3=df['MntGoldProds'].quantile(0.75)
IQR=Q3-Q1
lower_bound=0
upper_bound=Q3+1.5*IQR

gold_out=df[(df['MntGoldProds']<lower_bound) | (df['MntGoldProds']>upper_bound)]

gold_out['MntGoldProds']

x=np.sqrt(df['MntGoldProds'])
x.skew()

y=np.log1p(df['MntGoldProds'])
y.skew()

y_original=np.exp(y)
y_original.skew()

df['MntGoldProds_transformed'] , fitted_lambda = stats.boxcox(df['MntGoldProds'] + 1)  # added 1 if zeros exist

fitted_lambda

df['MntGoldProds_transformed'].skew()

visualize_column(df,'MntGoldProds_transformed')

df['MntGoldProds_transformed'].describe()

# Number of purchses made on offers
df['NumDealsPurchases']

df['NumDealsPurchases'].dtype

df['NumDealsPurchases'].describe()

df['NumDealsPurchases'].value_counts()

sns.countplot(x='NumDealsPurchases',data=df)

"""Most of the customers are purchasing one or 2 purchases on deals.
there are less numbers customers made more purchases during deals
"""

visualize_column(df,'NumDealsPurchases')

x=np.sqrt(df['NumDealsPurchases'])
x.skew()

y=np.log1p(df['NumDealsPurchases'])
y.skew()

df['NumDealsPurchases'].skew()

"""Box–Cox Transformation Formula
Works only if x > 0

y' = (x^λ - 1) / λ        , if λ ≠ 0

y' = ln(x)                , if λ = 0
"""

# WE can use the Yeo-johnson for this as it has 0's
df['NumDealsPurchases_transformed'] , fitted_lambda = stats.boxcox(df['NumDealsPurchases'] + 1)  # add 1 if zeros exist as the box-cox can't handle the 0's

fitted_lambda

df['NumDealsPurchases_transformed'].skew()

df['NumDealsPurchases_transformed'].describe()

visualize_column(df,'NumDealsPurchases_transformed')

df['NumWebPurchases']

df['NumWebPurchases'].dtype

df['NumWebPurchases'].describe()

df['NumWebPurchases'].unique()

sns.countplot(x='NumWebPurchases',data=df)

visualize_column(df,'NumWebPurchases')

df['NumWebPurchases'].skew()

df['NumWebPurchases_transformed']=np.sqrt(df['NumWebPurchases'])
df['NumWebPurchases_transformed'].skew()

y=np.log1p(df['NumWebPurchases'])
y.skew()

"""### Yeo–Johnson Transformation Formula
Works for both positive and negative values

For x ≥ 0:

  y' = ((x + 1)^λ - 1) / λ       , if λ ≠ 0

   y' = ln(x + 1)                 , if λ = 0

 For x < 0:

   y' = -(((-x + 1)^(2 - λ) - 1) / (2 - λ)) , if λ ≠ 2

   y' = -ln(-x + 1)                         , if λ = 2

"""

from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method='yeo-johnson')
x = pt.fit_transform(df[['NumWebPurchases']])

x=pd.DataFrame(x)

x.skew()

visualize_column(df,'NumWebPurchases_transformed')

df['NumWebPurchases_transformed'].describe()

# Number of times a customer bought through a product catalog (mail/phone order or printed list)
df['NumCatalogPurchases']

df['NumCatalogPurchases'].dtype

df['NumCatalogPurchases'].describe()

df['NumCatalogPurchases'].value_counts()

sns.countplot(x='NumCatalogPurchases',data=df)

visualize_column(df,'NumCatalogPurchases')

df['NumCatalogPurchases'].skew()

x=np.sqrt(df['NumCatalogPurchases'])
x.skew()

df['NumCatalogPurchases_transformed']=np.log1p(df['NumCatalogPurchases'])
df['NumCatalogPurchases_transformed'].skew()

df['NumCatalogPurchases_transformed'].describe()

visualize_column(df,'NumCatalogPurchases_transformed')

# Number of purchases made directly in stores.
df['NumStorePurchases']

df['NumStorePurchases'].dtype

df['NumStorePurchases'].info()

df['NumStorePurchases'].describe()

sns.countplot(x='NumStorePurchases',data=df)

visualize_column(df,'NumStorePurchases')

df['NumStorePurchases'].skew()

x=np.sqrt(df['NumStorePurchases'])
x.skew()

df['NumStorePurchases_transformed']=np.log1p(df['NumStorePurchases'])
df['NumStorePurchases_transformed'].skew()

visualize_column(df,'NumStorePurchases_transformed')

df['NumStorePurchases'].describe()

df['NumWebVisitsMonth']

df['NumWebVisitsMonth'].dtype

df['NumWebVisitsMonth'].describe()

sns.countplot(x='NumWebVisitsMonth',data=df)

visualize_column(df,'NumWebVisitsMonth')

df['NumWebVisitsMonth'].skew()

def transform_column_skew(df, col):
    """
    Transform a single numerical column to reduce skewness.
    Parameters:
    df : pandas.DataFrame
    col : str : column name to transform
    Returns:
    transformed_series : pd.Series
    info : dict with original skew, best skew, transformation applied
    """
    original_skew = df[col].skew()
    best_skew = original_skew
    best_transformation = 'None'
    transformed_series = df[col].copy()

    # Only transform if skew is outside [-0.5, 0.5]
    if original_skew < -0.5 or original_skew > 0.5:
        transformed_options = {}
        # 1. Square root (non-negative values only)
        if (df[col] >= 0).all():
            sqrt_transformed = np.sqrt(df[col])
            transformed_options['sqrt'] = sqrt_transformed.skew()
        # 2. log1p (log(1+x), x >= -1)
        if (df[col] >= -1).all():
            log1p_transformed = np.log1p(df[col])
            transformed_options['log1p'] = log1p_transformed.skew()
        # 3. Yeo-Johnson
        pt = PowerTransformer(method='yeo-johnson', standardize=False)
        yeo_transformed = pt.fit_transform(df[[col]])
        yeo_transformed = pd.Series(yeo_transformed.flatten(), index=df.index)
        transformed_options['yeo-johnson'] = yeo_transformed.skew()
        # Pick transformation closest to 0
        best_transformation, best_skew = min(transformed_options.items(), key=lambda x: abs(x[1]))
        # Apply the best transformation
        if best_transformation == 'sqrt':
            transformed_series = np.sqrt(df[col])
        elif best_transformation == 'log1p':
            transformed_series = np.log1p(df[col])
        elif best_transformation == 'yeo-johnson':
            transformed_series = yeo_transformed
    info = {
        'original_skew': original_skew,
        'best_skew': best_skew,
        'transformation': best_transformation
    }

    return transformed_series, info

# Suppose our DataFrame is df and column to transform is 'NumWebVisitsMonth'
transformed_col, report = transform_column_skew(df, 'NumWebVisitsMonth')

print(f"Original skew: {report['original_skew']:.2f}")
print(f"Best skew: {report['best_skew']:.2f}")
print(f"Transformation applied: {report['transformation']}")

# Replace original column with transformed data
df['NumWebVisitsMonth_transformed'] = transformed_col

df.columns

df['AcceptedCmp1']

df['AcceptedCmp1'].dtype



df['AcceptedCmp1'].value_counts()

sns.countplot(x='AcceptedCmp1',data=df,palette='viridis',edgecolor='black')
for p in plt.gca().patches:
    height = p.get_height()
    plt.gca().annotate(f'{height}', (p.get_x() + p.get_width()/2., height),
                       ha='center', va='bottom', fontsize=11)
plt.show()

"""From the above most of the customers did not accept the first campaign offers"""

df['AcceptedCmp2']

df['AcceptedCmp2'].dtype

df['AcceptedCmp2'].isnull().sum()

df['AcceptedCmp2'].value_counts()

sns.countplot(x='AcceptedCmp2',data=df,palette='viridis',edgecolor='black')
for p in plt.gca().patches:
    height = p.get_height()
    plt.gca().annotate(f'{height}', (p.get_x() + p.get_width()/2., height),
                       ha='center', va='bottom', fontsize=11)
plt.show()

"""From the above most of the customers did not accept the 2nd campaign"""

df['AcceptedCmp3']

df['AcceptedCmp3'].dtype

sns.countplot(x='AcceptedCmp3',data=df,palette='viridis',edgecolor='black')
for p in plt.gca().patches:
    height = p.get_height()
    plt.gca().annotate(f'{height}', (p.get_x() + p.get_width()/2., height),
                       ha='center', va='bottom', fontsize=11)
plt.show()

"""From the above most of the customers did not accept the third campaign but much better than first two



"""

df['AcceptedCmp4']

df['AcceptedCmp4'].dtype

sns.countplot(x='AcceptedCmp4',data=df,palette='viridis',edgecolor='black')
for p in plt.gca().patches:
    height = p.get_height()
    plt.gca().annotate(f'{height}', (p.get_x() + p.get_width()/2., height),
                       ha='center', va='bottom', fontsize=11)
plt.show()

"""From the above most of the customers did not accept the fourt campaign,but better than before.


"""

df['AcceptedCmp5']

df['AcceptedCmp5'].dtype

df['AcceptedCmp5'].describe()

sns.countplot(x='AcceptedCmp5',data=df,palette='viridis',edgecolor='black')
for p in plt.gca().patches:
    height = p.get_height()
    plt.gca().annotate(f'{height}', (p.get_x() + p.get_width()/2., height),
                       ha='center', va='bottom', fontsize=11)
plt.show()

"""From the above most of the customers did not accept the 5th campaign,little less than 4th.

**From All the 5 campaigns the second campaign has less  acceptences of the offer and mostly cutomers did not accept the offers during campaign**
"""

# Response to the most recent campaign (1 = accepted, 0 = not accepted). This is often the target variable in prediction tasks.
df['Response']

df['Response'].dtype

df['Response'].value_counts()

sns.countplot(x='Response',data=df,palette=['#FF4500','#FFD700'],edgecolor='black')
for p in plt.gca().patches:
    height = p.get_height()
    plt.gca().annotate(f'{height}', (p.get_x() + p.get_width()/2., height),
                       ha='center', va='bottom', fontsize=11)
plt.show()

"""From the recent campaign the customer acceptence of offer is increased by double this means the recent campaign attracts the customers withing their needs a lit bit than other campaigns .

"""

# 1 if the customer has complained in the last 2 years, 0 otherwise.
df['Complain']

df['Complain'].dtype

df['Complain'].value_counts()



counts = df['Complain'].value_counts()

# Labels
labels = counts.index.astype(str)  # '0' and '1'
# Colors (yellow for 0, red for 1)
colors = ['#FFD700', '#FF4500']  # yellow, red
# Create pie chart
plt.figure(figsize=(6,6))
plt.pie(
    counts,
    labels=labels,
    autopct='%1.1f%%',  # show percentage
    startangle=90,      # rotate start
    colors=colors,
    wedgeprops={'edgecolor':'black'}  # border around slices
)

plt.title('Distribution of Complain', fontsize=14)
plt.show()

"""From the above there are less than 1% of complaints have been raised this maeans the marketting is going well and but we have to check the complaints nce if there are any major complaints in them.

Any way the marketing was going well

"""

# Cost of a contact to the company .
df['Z_CostContact']

df['Z_CostContact'].dtype

df['Z_CostContact'].value_counts()

df['Z_CostContact'].unique()

"""This column has all values are same as it is has contact cost for all coustomers as 3  so we can drop this column during feature engineering as doesn't give much informstion for clustering"""

# Revenue from a single sale contact
df['Z_Revenue']

df['Z_Revenue'].dtype

df['Z_Revenue'].value_counts()

sns.countplot(x='Z_Revenue',data=df,palette='viridis',edgecolor='black')

"""The Revenue column also have same value as 11 as it the company gets revenue from the each customer is 11.
So we can drop it during the feature engineering or now
"""

# Dropping the columns Z_CostContact and Z_Revenue  and Id Which have unique values
df.drop(columns={'ID','Z_CostContact','Z_Revenue'},inplace=True)

"""# Bivariate analysis"""

df.head()

df.dtypes

df.corr(numeric_only=True)

plt.figure(figsize=(15,10))
sns.heatmap(df.corr(numeric_only=True),annot=True,cmap='coolwarm',linewidth=0.5)
plt.show()

"""From the above correlation heatmap we can find some Insights are:
* The customer with more income are mostly buying the Meat and Wines aand they prefer purchasing through catalog
* Most income Customers moderately buys the Fishes,sweets ,gold And Moderately uses the purchasing via store and web.
* And The Income is -vely correlated with Numofwebvistit.
* The kids home are mostly the more the kids the less the purchase amount on the wines ,meat, fruits ,fish ,sweet and the custoomers with more kids are not making purchases through the web,catalog,store.But they preffer deal purchases but in less.
* The Customers who purchases the meat are most like buying the wines. And The both Meat and Wines are most likely brought throgh the catalog mean The more catalog purchases the more amount of purchases on meat and wines.
* The wines and meat purchases are more in store and the web,This means lots of customers are purchasing meat and wines and fruits in every purchase platform.
* The more the webVisitmonth the less the purchase of the fruits,sweet,gold,wines,and meat.They mostly purchasing during the deals (offers)
* The more customer with more kid are mostly buying in the deals(offers)
* And Some are like storepurchases are moderately +vely correlated with webpurchases,catlogpurchases.
"""

df.columns

df['Total_Purchases']=df['MntFishProducts']+df['MntFruits']+df['MntGoldProds']+df['MntMeatProducts']+df['MntSweetProducts']+df['MntWines']

df['Total_Purchases'].head()

df['Total_Purchases'].describe()

visualize_column(df,'Total_Purchases')

df['Total_Purchases'].skew()

# Suppose our DataFrame is df and column to transform is 'NumWebVisitsMonth'
transformed_col, report = transform_column_skew(df, 'Total_Purchases')

print(f"Original skew: {report['original_skew']:.2f}")
print(f"Best skew: {report['best_skew']:.2f}")
print(f"Transformation applied: {report['transformation']}")

df['Total_Purchases_transformed']=transformed_col

visualize_column(df,'Total_Purchases_transformed')

df['Total_Purchases_transformed'].describe()

df['Total_Num_Purchases']=df['NumWebPurchases']+df['NumCatalogPurchases']+df['NumStorePurchases']+df['NumDealsPurchases']

df['Total_Num_Purchases'].head()

df['Total_Num_Purchases'].describe()

visualize_column(df,'Total_Num_Purchases')

df['Total_Num_Purchases'].skew()





r=df['Total_Num_Purchases'].corr(df['Total_Purchases_transformed'])
r

#VIF

vif=1/(1-r**2)
vif

"""Here the VIF for these two columns is 4.2 < 5 ,so we have to remove one column of create new column"""

# creating new column purchase ration
df['Purchases_Ratio'] = df['Total_Purchases'] / (df['Total_Num_Purchases'] + 1) # here we are adding 1 from preventing error by 0,as there may be 0 Total_num_purchases

df['Purchases_Ratio']

df['Purchases_Ratio'].describe()

visualize_column(df,'Purchases_Ratio')

df['Purchases_Ratio'].skew()

# Suppose our DataFrame is df and column to transform is 'NumWebVisitsMonth'
transformed_col, report = transform_column_skew(df, 'Purchases_Ratio')

print(f"Original skew: {report['original_skew']:.2f}")
print(f"Best skew: {report['best_skew']:.2f}")
print(f"Transformation applied: {report['transformation']}")

df['Purchases_Ratio_transformed']=transformed_col

df['Purchases_Ratio_transformed'].describe()

visualize_column(df,'Purchases_Ratio_transformed')

plt.figure(figsize=(20,15))
sns.heatmap(df.corr(numeric_only=True),annot=True,cmap='coolwarm',linewidth=0.5)
plt.show()

"""here we can drop the year_birt;h"""

df.head()

sns.boxplot(x='Marital_Status', y='Income', data=df)
plt.title("Income distribution across Marital Status")
plt.show()

"""Here almost martial status has the same distribution of income for every one but the together and married have the highest income among all,and there are less number of widows in the data."""



sns.boxplot(x='Education', y='Income', data=df)
plt.title("Income distribution across  Education")
plt.show()

"""From the above the PHD and graduates have more income and most of the salaries are range from 20000 to 700000 and the Basic Education has lowest salary."""

cr=pd.crosstab(df['Marital_Status'],df['Education'])
cr.plot(kind='bar',stacked='True')

"""From the above chart we can find that are more Graduates in every martial status married"""

sns.boxplot(x='Response', y='Income', data=df)
plt.title("Response distribution across  Income")
plt.show()

"""From this the income ranges from 50 to 80k are mostly gave +ve Response and the income from 20 to 60 k are mostly have -ve response and the customers with high income almost decline the campaign"""

ttest_ind(df[df['Response']==1]['Income'],df[df['Response']==0]['Income'])

"""Here the p<0.05 so these two features are significant,where these two are statastically different,means they carry different information."""

df['Customer_Since_Days'] = (pd.Timestamp.today() - df['Dt_Customer']).dt.days

df[['Customer_Since_Days', 'Age','Income','Purchases_Ratio']].corr()

sns.scatterplot(x='Customer_Since_Days', y='Income', data=df)

df['Year']=df['Dt_Customer'].dt.year
# Count number of customers per year
customers_per_year = df['Year'].value_counts().sort_index().reset_index()
customers_per_year.columns = ['Year', 'Num_Customers']
# Display the result
print(customers_per_year)

# Plot the trend
plt.figure(figsize=(8,5))
plt.plot(customers_per_year['Year'], customers_per_year['Num_Customers'], marker='o')
plt.title('Number of Customers by Year')
plt.xlabel('Year')
plt.ylabel('Number of Customers')
plt.grid(True)
plt.show()

sns.boxplot(x='Education', y='Customer_Since_Days', data=df)
plt.title("Response distribution across  Income")
plt.show()

"""From the above plot the basic education level customers have joined long ago than others"""

# Age vs Recency
sns.scatterplot(x='Age', y='Recency', data=df)
plt.title('Age vs Recency')
plt.show()

# Campaigns vs Recency
sns.boxplot(x='Response', y='Recency', data=df)
plt.title('Recency by Campaign Participation')
plt.show()

recency_0 = df[df['Response'] == 0]['Recency']
recency_1 = df[df['Response'] == 1]['Recency']
ttest_ind(recency_0, recency_1, equal_var=False)

"""Customers who accepted the campaign have significantly higher recency values,

-> They made their last purchase a long time ago (they are less recent customers).

-> Customers who did not accept the campaign are more recent buyers.
"""

campaign_cols = [col for col in df.columns if 'acceptedcmp' in col.lower()]
print(campaign_cols)
for col in campaign_cols:
    plt.figure(figsize=(5,4))
    sns.boxplot(x=col, y='Recency', data=df)
    plt.title(f"Recency by {col}")
    plt.show()

for col in campaign_cols:
    rec0 = df[df[col] == 0]['Recency']
    rec1 = df[df[col] == 1]['Recency']
    t_stat, p_val = ttest_ind(rec0, rec1, equal_var=False)
    print(f"{col}: t={t_stat:.3f}, p={p_val:.3f}")

"""There is no clear evidence that recency (how recent the last purchase was) affects whether a customer accepts any campaign."""



education_order = {
    'Basic': 1,
    '2n Cycle': 2,
    'Graduation': 3,
    'Master': 4,
    'PhD': 5
}

df['Education_Ordinal'] = df['Education'].map(education_order)

df=pd.get_dummies(df,columns=['Marital_Status'],drop_first=True,dtype=int)

df['campaign']=df['AcceptedCmp1']+df['AcceptedCmp2']+df['AcceptedCmp3']+df['AcceptedCmp4']+df['AcceptedCmp5']+df['Response']

df.head()

df.columns

EDA_df=df[['Income','Complain', 'Response', 'Age', 'FamilySize', 'MntWines_transformed',
       'MntFruits_transformed', 'MntMeatProducts_transformed',
       'MntFishProducts_transformed', 'MntSweetProducts_transformed',
       'MntGoldProds_transformed', 'NumDealsPurchases_transformed',
       'NumWebPurchases_transformed', 'NumCatalogPurchases_transformed',
       'NumStorePurchases_transformed', 'NumWebVisitsMonth_transformed',
        'Total_Purchases_transformed', 'Total_Num_Purchases',
      'Purchases_Ratio_transformed', 'Customer_Since_Days',
       'Education_Ordinal', 'Marital_Status_Married',
       'Marital_Status_Single', 'Marital_Status_Together',
       'Marital_Status_Widow', 'campaign']]

EDA_df.columns.duplicated().sum()

EDA_df

EDA_df.dtypes

EDA_df.corr()

plt.figure(figsize=(15,10))
sns.heatmap(EDA_df.corr(),annot=True,cmap='coolwarm',linewidth=0.5)
plt.show()

"""Dropping the features which are having correlation > 8.5"""

corr_matrix = EDA_df.corr().abs()
upper = corr_matrix.where(
    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
)

to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]
df_reduced = EDA_df.drop(columns=to_drop)
print("Dropped:", to_drop)

df_reduced

plt.figure(figsize=(15,10))
sns.heatmap(df_reduced.corr(),annot=True,cmap='coolwarm',linewidth=0.5)
plt.show()



"""# Multivariate Analysis"""

sns.pairplot(df_reduced)

df_reduced.dtypes

df_reduced.head()

df_reduced.to_csv("EDA_Data.csv")

df_reduced.columns

df_reduced.describe()

from sklearn.feature_selection import VarianceThreshold

sel = VarianceThreshold(threshold=0.01)
sel.fit(df_reduced)
low_variance = df_reduced.columns[~sel.get_support()]
print("Low variance features:", low_variance)

df_reduced.drop(columns=low_variance,inplace=True)

#scaling
from sklearn.preprocessing import StandardScaler,MinMaxScaler
sc=StandardScaler()
sc.fit(df_reduced)
standard_EDA_df=sc.transform(df_reduced)

# Applying scaling(Standard Scaler) for numerical data, the data comes as mean 0 and standard eviation to 1

standard_EDA_df=pd.DataFrame(standard_EDA_df,columns=df_reduced.columns,)

standard_EDA_df.describe()

standard_EDA_df.columns

corr = standard_EDA_df.corr()
plt.figure(figsize=(18,10))
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.show()

standard_EDA_df.reset_index(drop=True, inplace=True)

standard_EDA_df.shape

standard_EDA_df

from sklearn.decomposition import PCA
p=PCA(n_components=0.90)
p.fit(standard_EDA_df)

print("PCA expects:", p.n_features_in_)

"""weights of the original data features"""

W=p.components_.T
W

p.n_components_

p.explained_variance_ratio_

PCA_data=p.transform(standard_EDA_df)
PCA_data

# create column names (PC1, PC2, ...)
component_names = [f'PC{i+1}' for i in range(p.n_components_)]
component_names

PCA_data=pd.DataFrame(PCA_data,columns=component_names)

p.explained_variance_ratio_.cumsum()

sns.barplot(x = list(range(1,p.n_components_+1)), y = p.explained_variance_, palette = 'GnBu_r')
plt.xlabel('i')
plt.ylabel('Lambda i');

"""# Model Building

# AgglomerativeClustering
"""

from sklearn.cluster import AgglomerativeClustering
AC = AgglomerativeClustering(n_clusters=5)
# fit model and predict clusters
yhat_AC = AC.fit_predict(PCA_data)
PCA_data['Clusters'] = yhat_AC
#Adding the Clusters feature to the orignal dataframe.
standard_EDA_df['Clusters'] = yhat_AC

standard_EDA_df['Clusters'].value_counts()

from sklearn.metrics import silhouette_score,davies_bouldin_score
print("davies_bouldin_score",davies_bouldin_score(PCA_data,yhat_AC))
print("silhouette_score:",silhouette_score(PCA_data,yhat_AC))

from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt

Z = linkage(PCA_data, method='ward')  # 'ward' minimizes variance within clusters

plt.figure(figsize=(12, 6))
dendrogram(Z)
plt.title("Dendrogram for Hierarchical Clustering")
plt.xlabel("Samples")
plt.ylabel("Distance")
plt.show()

"""From the aboove dendogram the number of clusters are 2 (scear and distinct seperation )

WE cantake 3-4 for seperation of sub groups.

"""

import scipy.cluster.hierarchy as sch

dendogram=sch.dendrogram(sch.linkage(PCA_data,method='centroid'))
plt.show()

linkages = ['ward', 'complete', 'average','single']
metric = ['euclidean', 'manhattan', 'cosine']
n_clusters = [3,4,5]
best_score = -1
best_params = {}
for link in linkages:
    for n in n_clusters:
  # Ward linkage works only with Euclidean
        if link == 'ward':
            affs = ['euclidean']
        else:
            affs = metric

        for aff in affs:
            model = AgglomerativeClustering(
                n_clusters=n, linkage=link, metric=aff
            )
            labels = model.fit_predict(PCA_data)
            score = silhouette_score(PCA_data, labels)
            if score > best_score:
                best_score = score
                best_params = {'n_clusters': n, 'linkage': link, 'metric': aff}

print(f"Best Score: {best_score:.3f}")
print("Best Parameters:", best_params)

agg_model=AgglomerativeClustering(n_clusters=4, linkage='complete', metric='manhattan')
labels = agg_model.fit_predict(PCA_data)
agg_score = silhouette_score(PCA_data, labels)
agg_score

scores={}
scores['agg_score']=agg_score
scores

davies_bouldin_score(PCA_data,labels)

unique,counts=np.unique(labels,return_counts=True)
value_counts=dict(zip(unique,counts))
value_counts



"""# KMeans"""

from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN
from yellowbrick.cluster import KElbowVisualizer
Elbow_M = KElbowVisualizer(KMeans(), k=10)
Elbow_M.fit(PCA_data)
Elbow_M.show();

'''from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN
from yellowbrick.cluster import KElbowVisualizer
Elbow_M = KElbowVisualizer(KMeans(), k=10,metric='silhouette')
Elbow_M.fit(PCA_data)
Elbow_M.show()''';

wcss=[]
for i in range(2,20):
    kmeans_model=KMeans(n_clusters=i).fit(PCA_data)
    wcss.append(kmeans_model.inertia_)
wcss

plt.plot(range(2,20),wcss)
plt.show()

"""From this WCSS graph the sharp drop from k=4 or 5,THe best elbow curve is at 5"""

inits = ['k-means++', 'random']
n_init_values = [10, 20,25,30,40,45, 50]
best_score = -1
best_params = {}
for init_method in inits:
    for n_init_val in n_init_values:
        kmeans = KMeans(n_clusters=5, init=init_method, n_init=n_init_val, max_iter=300, random_state=42)
        labels = kmeans.fit_predict(PCA_data)
        score = silhouette_score(PCA_data, labels)
        if score > best_score:
            best_score = score
            best_params = {'init': init_method, 'n_init': n_init_val}
print(f"Best Silhouette Score: {best_score:.3f}")
print("Best Parameters:", best_params)

davies_bouldin_score(PCA_data,labels)

kmeans_final_model=KMeans(n_clusters=5,init='k-means++',n_init=45,max_iter=300,random_state=42)
kmeans_final_model.fit(PCA_data)

clusters=kmeans_final_model.predict(PCA_data)

unique,counts=np.unique(clusters,return_counts=True)
value_counts=dict(zip(unique,counts))
value_counts

kmeans_score=silhouette_score(PCA_data,clusters)
kmeans_score

scores['kamean_score']=kmeans_score
scores

plt.figure(figsize=(8,6))
plt.scatter(PCA_data.iloc[:, 1], PCA_data.iloc[:, 2],
            c=clusters, cmap='viridis', s=50, alpha=0.7)

plt.figure(figsize=(8,6))
plt.scatter(PCA_data.iloc[:, 0], PCA_data.iloc[:, 1],
            c=clusters, cmap='viridis', s=50, alpha=0.7)

# Plot centroids
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1],
            c='red', s=200, marker='X', label='Centroids')

plt.title(f'K-Means Clusters (k={5}) on PCA Data')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

"""# DBSCAN"""

from sklearn.neighbors import NearestNeighbors
# Choose min_samples first
min_samples = 5
# Fit NearestNeighbors
neigh = NearestNeighbors(n_neighbors=min_samples)
nbrs = neigh.fit(PCA_data)
distances, indices = nbrs.kneighbors(PCA_data)

# Sort distances (the distance to the k-th nearest neighbor)
distances = np.sort(distances[:, min_samples - 1])

# Plot k-distance graph
plt.figure(figsize=(8,5))
plt.plot(distances)
plt.title("k-Distance Graph for DBSCAN")
plt.xlabel("Data Points sorted by distance")
plt.ylabel(f"{min_samples}-NN Distance")
plt.show()

"""From the above k-Distance graph the sharp rise beyon 2.5 to 3.0.

So the eps can be from 2.5 to 3.0
"""

dbscan=DBSCAN(eps=2.5,min_samples=PCA_data.shape[1]+1)

labels = dbscan.fit_predict(PCA_data)

unique,counts=np.unique(labels,return_counts=True)
value_counts=dict(zip(unique,counts))
value_counts

# Check how many clusters were formed
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
print("Estimated clusters:", n_clusters)
print("Noise points:", list(labels).count(-1))

for eps in [2.0,2.3,2.5, 2.6,2.7,2.8,2.9,3.0,3.1,3.2,3.3]:
    dbscan = DBSCAN(eps=eps, min_samples=11)
    labels = dbscan.fit_predict(PCA_data)
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = list(labels).count(-1)
    print(f"eps={eps:.4f} → clusters={n_clusters}, noise={n_noise}")

eps_values = [2.7,2.8,2.9,3.0]
min_samples_values =np.arange(5,15)

best_score = -1
best_params = {}

for eps in eps_values:
    for ms in min_samples_values:
        model = DBSCAN(eps=eps, min_samples=ms, metric='euclidean')
        labels = model.fit_predict(PCA_data)

        # Ignore if all points are in one cluster or only noise
        if len(set(labels)) <= 1:
            continue

        score = silhouette_score(PCA_data, labels)
        if score > best_score:
            best_score = score
            best_params = {'eps': eps, 'min_samples': ms}
print(f"Best Silhouette Score: {best_score:.3f}")
print("Best Parameters:", best_params)

unique,counts=np.unique(labels,return_counts=True)
value_counts=dict(zip(unique,counts))
value_counts

dbscan_model = DBSCAN(eps=3.0, min_samples=6, metric='euclidean')
clusters = model.fit_predict(PCA_data)

dbscan_score=silhouette_score(PCA_data,clusters)
dbscan_score

scores['dbscan_score']=dbscan_score
scores

davies_bouldin_score(PCA_data,clusters)

unique,counts=np.unique(clusters,return_counts=True)
value_counts=dict(zip(unique,counts))
value_counts

plt.figure(figsize=(8,6))
plt.scatter(PCA_data.iloc[:, 0], PCA_data.iloc[:, 1],
            c=clusters, cmap='viridis', s=50, alpha=0.7)



"""# OPTICS"""

from sklearn.cluster import OPTICS

model = OPTICS(min_samples=5, xi=0.05, min_cluster_size=0.05)
labels = model.fit_predict(PCA_data)

plt.figure(figsize=(10, 5))
plt.plot(model.reachability_[model.ordering_], 'r-')
plt.title('Reachability Plot (OPTICS)')
plt.xlabel('Sample Index')
plt.ylabel('Reachability Distance')
plt.show()

"""The reachability distance increases sharply after around 2.5–3.0

It looks like there are 4-6 clusters .
"""

from sklearn.cluster import OPTICS
from collections import Counter

# Try a few parameter sets
for xi in [0.03, 0.05, 0.07, 0.1]:
    for min_samples in [5, 10, 15]:
        optics = OPTICS(min_samples=min_samples, xi=xi, max_eps=3)
        optics.fit(PCA_data)
        labels = optics.labels_
        counts = Counter(labels)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        print(f"xi={xi}, min_samples={min_samples} → clusters={n_clusters}, noise={counts[-1] if -1 in counts else 0}")

optics_model = OPTICS(min_samples=10, xi=0.1,max_eps=3)
labels = optics_model.fit_predict(PCA_data)
opt_score=silhouette_score(PCA_data,labels)
opt_score

unique,counts=np.unique(labels,return_counts=True)
value_counts=dict(zip(unique,counts))
value_counts

scores['opt_score']=opt_score

"""# GaussianMixture"""

from sklearn.mixture import GaussianMixture

bics = []
sil_scores = []

for k in range(2, 10):
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm.fit(PCA_data)
    labels = gmm.predict(PCA_data)
    bics.append(gmm.bic(PCA_data))
    sil_scores.append(silhouette_score(PCA_data, labels))

plt.plot(range(2, 10), bics, 'r^-')
plt.xlabel('Components')
plt.ylabel('BIC Score')
plt.title('GMM BIC Scores')
plt.show()

plt.plot(range(2, 10), sil_scores, 'b*-')
plt.xlabel('Components')
plt.ylabel('Silhouette Score')
plt.title('GMM Silhouette Scores')
plt.show()

"""The optimal number of clusters for GMM is 2.

the highest silhouette score is at 2 components, but the BIC suggests 8-9

to balance both the clusters are 3-4
"""

# Fit GMM with best number of components
best_n= 3  # from BIC curve
gmm = GaussianMixture(n_components=best_n, random_state=42)
labels = gmm.fit_predict(PCA_data)

# Plot clusters
plt.figure(figsize=(8,6))
plt.scatter(PCA_data.iloc[:, 0], PCA_data.iloc[:, 1],
            c=labels, cmap='viridis', s=50, alpha=0.7)
plt.title(f"GMM Clustering (n_components={best_n})")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.show()

unique,counts=np.unique(labels,return_counts=True)
value_counts=dict(zip(unique,counts))
value_counts

cov_types = ['full', 'tied', 'diag', 'spherical']
best_score = -1
best_cov = None

for cov in cov_types:
    gmm = GaussianMixture(n_components=4, covariance_type=cov, random_state=42)
    labels = gmm.fit_predict(PCA_data)
    score = silhouette_score(PCA_data, labels)
    print(f"{cov} - Silhouette Score: {score:.3f}")
    if score > best_score:
        best_score = score
        best_cov = cov

print("Best covariance type:", best_cov)

best_gmm = GaussianMixture(n_components=4, covariance_type='spherical', random_state=42)
labels = best_gmm.fit_predict(PCA_data)

gmm_score = silhouette_score(PCA_data, labels)
print("Final Silhouette Score:", gmm_score)

unique,counts=np.unique(labels,return_counts=True)
value_counts=dict(zip(unique,counts))
value_counts

scores['gmm_score']=gmm_score

scores

"""From all the above clustering models the optics got best silhouette score but the clusters are imbalances as the most of the data pionts are under same cluster,So we don't take it.after that the k-means performs well with well defined clusters and better silhoutte score ,then the final models is **K-Means**"""

final_model=KMeans(n_clusters=5,init='k-means++',n_init=45,max_iter=300,random_state=42)
final_model.fit(PCA_data)

clusters=final_model.predict(PCA_data)

EDA_df['clusters']=clusters

pca_vis=PCA(n_components=2)
pca_vis.fit(PCA_data)
PCA_data_vis=pca_vis.transform(PCA_data)
PCA_data_vis=pd.DataFrame(PCA_data_vis,columns=['pca1','pca2'])
PCA_data_vis['clusters']=clusters
plt.figure(figsize=(8,6))
plt.scatter(PCA_data_vis['pca1'], PCA_data_vis['pca2'],
            c=clusters, cmap='viridis', s=50, alpha=0.7)

plt.figure(figsize=(10,8))

plt.scatter(
    PCA_data_vis['pca1'],
    PCA_data_vis['pca2'],
    c=clusters,
    cmap='tab10',     # Much clearer, distinct colors
    s=70,             # Bigger points
    alpha=0.9,        # More visible
    edgecolor="black", # Outline around points
    linewidth=0.3
)

plt.title("Cluster Visualization using PCA", fontsize=16)
plt.xlabel("PCA 1", fontsize=14)
plt.ylabel("PCA 2", fontsize=14)
plt.grid(True, linestyle='--', alpha=0.3)
plt.show()

EDA_df['clusters'].unique()

EDA_df.to_csv('EDA_df_clusters.csv')

df.iloc[0]

"""# Deployment"""

import joblib
# Save your preprocessing pipeline and final model
joblib.dump(sc, "scaler.pkl")          # StandardScaler
joblib.dump(p, "pca.pkl")              # PCA

joblib.dump(final_model,'final_model.pkl')









